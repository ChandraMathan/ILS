{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn_agent import Agent\n",
    "from env_setup import EnvGrid\n",
    "from env_setup import EnvWeb\n",
    "\n",
    "#when using grid environment use below lines to train\n",
    "\"\"\" \n",
    "env = EnvGrid (8 , 4)\n",
    "#agent = Agent(state_size=6, action_size=2, seed=0)\n",
    "agent = Agent(state_size=9, action_size=34, seed=2)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#when traiing on webpage use this:\n",
    "state_size = 8 #this is approx number of input fields + label of interest\n",
    "\n",
    "\n",
    "num_vertical_grid = 10  #this should match speified in 'integration.ipynb'\n",
    "num_horizontal_grid = 10 #this should match speified in 'integration.ipynb'\n",
    "\n",
    "action_size = num_vertical_grid * num_horizontal_grid\n",
    "\n",
    "env = EnvWeb ()\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5000\tAverage Score: -0.40\n",
      "\n",
      "Episode 5000\tAverage Score: -0.40 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 10000\tAverage Score: -0.38\n",
      "\n",
      "Episode 10000\tAverage Score: -0.38 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 15000\tAverage Score: -0.28\n",
      "\n",
      "Episode 15000\tAverage Score: -0.28 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 20000\tAverage Score: -0.17\n",
      "\n",
      "Episode 20000\tAverage Score: -0.17 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 25000\tAverage Score: -0.33\n",
      "\n",
      "Episode 25000\tAverage Score: -0.33 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 30000\tAverage Score: -0.21\n",
      "\n",
      "Episode 30000\tAverage Score: -0.21 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 35000\tAverage Score: -0.31\n",
      "\n",
      "Episode 35000\tAverage Score: -0.31 0.3\n",
      "state:  [62 42 44 48 55 73 77 92]\n",
      "action:  8\n",
      "Episode 40000\tAverage Score: -0.22\n",
      "\n",
      "Episode 40000\tAverage Score: -0.22 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  77\n",
      "Episode 45000\tAverage Score: -0.21\n",
      "\n",
      "Episode 45000\tAverage Score: -0.21 0.3\n",
      "state:  [65 42 44 48 55 73 77 92]\n",
      "action:  98\n",
      "Episode 50000\tAverage Score: -0.32\n",
      "\n",
      "Episode 50000\tAverage Score: -0.32 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  77\n",
      "Episode 55000\tAverage Score: -0.14\n",
      "\n",
      "Episode 55000\tAverage Score: -0.14 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  77\n",
      "Episode 60000\tAverage Score: -0.19\n",
      "\n",
      "Episode 60000\tAverage Score: -0.19 0.3\n",
      "state:  [37 42 44 48 55 73 77 92]\n",
      "action:  98\n",
      "Episode 65000\tAverage Score: -0.23\n",
      "\n",
      "Episode 65000\tAverage Score: -0.23 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 70000\tAverage Score: -0.14\n",
      "\n",
      "Episode 70000\tAverage Score: -0.14 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  55\n",
      "Episode 75000\tAverage Score: -0.33\n",
      "\n",
      "Episode 75000\tAverage Score: -0.33 0.3\n",
      "state:  [81 42 44 48 55 73 77 92]\n",
      "action:  39\n",
      "Episode 80000\tAverage Score: -0.27\n",
      "\n",
      "Episode 80000\tAverage Score: -0.27 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  44\n",
      "Episode 85000\tAverage Score: -0.24\n",
      "\n",
      "Episode 85000\tAverage Score: -0.24 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 90000\tAverage Score: -0.27\n",
      "\n",
      "Episode 90000\tAverage Score: -0.27 0.3\n",
      "state:  [62 42 44 48 55 73 77 92]\n",
      "action:  87\n",
      "Episode 95000\tAverage Score: -0.16\n",
      "\n",
      "Episode 95000\tAverage Score: -0.16 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 100000\tAverage Score: -0.24\n",
      "\n",
      "Episode 100000\tAverage Score: -0.24 0.3\n",
      "state:  [81 42 44 48 55 73 77 92]\n",
      "action:  79\n",
      "Episode 105000\tAverage Score: -0.19\n",
      "\n",
      "Episode 105000\tAverage Score: -0.19 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 110000\tAverage Score: -0.28\n",
      "\n",
      "Episode 110000\tAverage Score: -0.28 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  48\n",
      "Episode 115000\tAverage Score: -0.25\n",
      "\n",
      "Episode 115000\tAverage Score: -0.25 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  44\n",
      "Episode 120000\tAverage Score: -0.23\n",
      "\n",
      "Episode 120000\tAverage Score: -0.23 0.3\n",
      "state:  [31 42 44 48 55 73 77 92]\n",
      "action:  26\n",
      "Episode 125000\tAverage Score: -0.24\n",
      "\n",
      "Episode 125000\tAverage Score: -0.24 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  55\n",
      "Episode 130000\tAverage Score: -0.23\n",
      "\n",
      "Episode 130000\tAverage Score: -0.23 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 135000\tAverage Score: -0.21\n",
      "\n",
      "Episode 135000\tAverage Score: -0.21 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  44\n",
      "Episode 140000\tAverage Score: -0.35\n",
      "\n",
      "Episode 140000\tAverage Score: -0.35 0.3\n",
      "state:  [62 42 44 48 55 73 77 92]\n",
      "action:  84\n",
      "Episode 145000\tAverage Score: -0.35\n",
      "\n",
      "Episode 145000\tAverage Score: -0.35 0.3\n",
      "state:  [37 42 44 48 55 73 77 92]\n",
      "action:  61\n",
      "Episode 150000\tAverage Score: -0.32\n",
      "\n",
      "Episode 150000\tAverage Score: -0.32 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 155000\tAverage Score: -0.39\n",
      "\n",
      "Episode 155000\tAverage Score: -0.39 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  77\n",
      "Episode 160000\tAverage Score: -0.24\n",
      "\n",
      "Episode 160000\tAverage Score: -0.24 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  44\n",
      "Episode 165000\tAverage Score: -0.23\n",
      "\n",
      "Episode 165000\tAverage Score: -0.23 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 170000\tAverage Score: -0.28\n",
      "\n",
      "Episode 170000\tAverage Score: -0.28 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 175000\tAverage Score: -0.16\n",
      "\n",
      "Episode 175000\tAverage Score: -0.16 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  77\n",
      "Episode 180000\tAverage Score: -0.22\n",
      "\n",
      "Episode 180000\tAverage Score: -0.22 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  77\n",
      "Episode 185000\tAverage Score: -0.30\n",
      "\n",
      "Episode 185000\tAverage Score: -0.30 0.3\n",
      "state:  [62 42 44 48 55 73 77 92]\n",
      "action:  40\n",
      "Episode 190000\tAverage Score: -0.23\n",
      "\n",
      "Episode 190000\tAverage Score: -0.23 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  77\n",
      "Episode 195000\tAverage Score: -0.22\n",
      "\n",
      "Episode 195000\tAverage Score: -0.22 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  48\n",
      "Episode 200000\tAverage Score: -0.20\n",
      "\n",
      "Episode 200000\tAverage Score: -0.20 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 205000\tAverage Score: -0.19\n",
      "\n",
      "Episode 205000\tAverage Score: -0.19 0.3\n",
      "state:  [65 42 44 48 55 73 77 92]\n",
      "action:  16\n",
      "Episode 210000\tAverage Score: -0.21\n",
      "\n",
      "Episode 210000\tAverage Score: -0.21 0.3\n",
      "state:  [31 42 44 48 55 73 77 92]\n",
      "action:  78\n",
      "Episode 215000\tAverage Score: -0.23\n",
      "\n",
      "Episode 215000\tAverage Score: -0.23 0.3\n",
      "state:  [37 42 44 48 55 73 77 92]\n",
      "action:  50\n",
      "Episode 220000\tAverage Score: -0.21\n",
      "\n",
      "Episode 220000\tAverage Score: -0.21 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 225000\tAverage Score: -0.19\n",
      "\n",
      "Episode 225000\tAverage Score: -0.19 0.3\n",
      "state:  [81 42 44 48 55 73 77 92]\n",
      "action:  16\n",
      "Episode 230000\tAverage Score: -0.21\n",
      "\n",
      "Episode 230000\tAverage Score: -0.21 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 235000\tAverage Score: -0.15\n",
      "\n",
      "Episode 235000\tAverage Score: -0.15 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 240000\tAverage Score: -0.23\n",
      "\n",
      "Episode 240000\tAverage Score: -0.23 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  77\n",
      "Episode 245000\tAverage Score: -0.15\n",
      "\n",
      "Episode 245000\tAverage Score: -0.15 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  48\n",
      "Episode 250000\tAverage Score: -0.19\n",
      "\n",
      "Episode 250000\tAverage Score: -0.19 0.3\n",
      "state:  [51 42 44 48 55 73 77 92]\n",
      "action:  75\n",
      "Episode 255000\tAverage Score: -0.19\n",
      "\n",
      "Episode 255000\tAverage Score: -0.19 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 260000\tAverage Score: -0.13\n",
      "\n",
      "Episode 260000\tAverage Score: -0.13 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 265000\tAverage Score: -0.24\n",
      "\n",
      "Episode 265000\tAverage Score: -0.24 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  55\n",
      "Episode 270000\tAverage Score: -0.32\n",
      "\n",
      "Episode 270000\tAverage Score: -0.32 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  55\n",
      "Episode 275000\tAverage Score: -0.25\n",
      "\n",
      "Episode 275000\tAverage Score: -0.25 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  55\n",
      "Episode 280000\tAverage Score: -0.23\n",
      "\n",
      "Episode 280000\tAverage Score: -0.23 0.3\n",
      "state:  [31 42 44 48 55 73 77 92]\n",
      "action:  61\n",
      "Episode 285000\tAverage Score: -0.21\n",
      "\n",
      "Episode 285000\tAverage Score: -0.21 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 290000\tAverage Score: -0.28\n",
      "\n",
      "Episode 290000\tAverage Score: -0.28 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 295000\tAverage Score: -0.27\n",
      "\n",
      "Episode 295000\tAverage Score: -0.27 0.3\n",
      "state:  [33 42 44 48 55 73 77 92]\n",
      "action:  1\n",
      "Episode 300000\tAverage Score: -0.23\n",
      "\n",
      "Episode 300000\tAverage Score: -0.23 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 305000\tAverage Score: -0.34\n",
      "\n",
      "Episode 305000\tAverage Score: -0.34 0.3\n",
      "state:  [33 42 44 48 55 73 77 92]\n",
      "action:  42\n",
      "Episode 310000\tAverage Score: -0.42\n",
      "\n",
      "Episode 310000\tAverage Score: -0.42 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 315000\tAverage Score: -0.31\n",
      "\n",
      "Episode 315000\tAverage Score: -0.31 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  55\n",
      "Episode 320000\tAverage Score: -0.17\n",
      "\n",
      "Episode 320000\tAverage Score: -0.17 0.3\n",
      "state:  [31 42 44 48 55 73 77 92]\n",
      "action:  90\n",
      "Episode 325000\tAverage Score: -0.28\n",
      "\n",
      "Episode 325000\tAverage Score: -0.28 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  44\n",
      "Episode 330000\tAverage Score: -0.38\n",
      "\n",
      "Episode 330000\tAverage Score: -0.38 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 335000\tAverage Score: -0.19\n",
      "\n",
      "Episode 335000\tAverage Score: -0.19 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 340000\tAverage Score: -0.26\n",
      "\n",
      "Episode 340000\tAverage Score: -0.26 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 345000\tAverage Score: -0.26\n",
      "\n",
      "Episode 345000\tAverage Score: -0.26 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 350000\tAverage Score: -0.35\n",
      "\n",
      "Episode 350000\tAverage Score: -0.35 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 355000\tAverage Score: -0.26\n",
      "\n",
      "Episode 355000\tAverage Score: -0.26 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 360000\tAverage Score: -0.16\n",
      "\n",
      "Episode 360000\tAverage Score: -0.16 0.3\n",
      "state:  [62 42 44 48 55 73 77 92]\n",
      "action:  96\n",
      "Episode 365000\tAverage Score: -0.23\n",
      "\n",
      "Episode 365000\tAverage Score: -0.23 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 370000\tAverage Score: -0.34\n",
      "\n",
      "Episode 370000\tAverage Score: -0.34 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  44\n",
      "Episode 375000\tAverage Score: -0.27\n",
      "\n",
      "Episode 375000\tAverage Score: -0.27 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 380000\tAverage Score: -0.26\n",
      "\n",
      "Episode 380000\tAverage Score: -0.26 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 385000\tAverage Score: -0.16\n",
      "\n",
      "Episode 385000\tAverage Score: -0.16 0.3\n",
      "state:  [65 42 44 48 55 73 77 92]\n",
      "action:  72\n",
      "Episode 390000\tAverage Score: -0.19\n",
      "\n",
      "Episode 390000\tAverage Score: -0.19 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  44\n",
      "Episode 395000\tAverage Score: -0.22\n",
      "\n",
      "Episode 395000\tAverage Score: -0.22 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  55\n",
      "Episode 400000\tAverage Score: -0.30\n",
      "\n",
      "Episode 400000\tAverage Score: -0.30 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 405000\tAverage Score: -0.17\n",
      "\n",
      "Episode 405000\tAverage Score: -0.17 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 410000\tAverage Score: -0.19\n",
      "\n",
      "Episode 410000\tAverage Score: -0.19 0.3\n",
      "state:  [81 42 44 48 55 73 77 92]\n",
      "action:  76\n",
      "Episode 415000\tAverage Score: -0.40\n",
      "\n",
      "Episode 415000\tAverage Score: -0.40 0.3\n",
      "state:  [62 42 44 48 55 73 77 92]\n",
      "action:  60\n",
      "Episode 420000\tAverage Score: -0.27\n",
      "\n",
      "Episode 420000\tAverage Score: -0.27 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 425000\tAverage Score: -0.26\n",
      "\n",
      "Episode 425000\tAverage Score: -0.26 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 430000\tAverage Score: -0.19\n",
      "\n",
      "Episode 430000\tAverage Score: -0.19 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  44\n",
      "Episode 435000\tAverage Score: -0.44\n",
      "\n",
      "Episode 435000\tAverage Score: -0.44 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 440000\tAverage Score: -0.27\n",
      "\n",
      "Episode 440000\tAverage Score: -0.27 0.3\n",
      "state:  [31 42 44 48 55 73 77 92]\n",
      "action:  32\n",
      "Episode 445000\tAverage Score: -0.28\n",
      "\n",
      "Episode 445000\tAverage Score: -0.28 0.3\n",
      "state:  [62 42 44 48 55 73 77 92]\n",
      "action:  14\n",
      "Episode 450000\tAverage Score: -0.20\n",
      "\n",
      "Episode 450000\tAverage Score: -0.20 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  48\n",
      "Episode 455000\tAverage Score: -0.31\n",
      "\n",
      "Episode 455000\tAverage Score: -0.31 0.3\n",
      "state:  [37 42 44 48 55 73 77 92]\n",
      "action:  35\n",
      "Episode 460000\tAverage Score: -0.16\n",
      "\n",
      "Episode 460000\tAverage Score: -0.16 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  77\n",
      "Episode 465000\tAverage Score: -0.24\n",
      "\n",
      "Episode 465000\tAverage Score: -0.24 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  44\n",
      "Episode 470000\tAverage Score: -0.25\n",
      "\n",
      "Episode 470000\tAverage Score: -0.25 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  48\n",
      "Episode 475000\tAverage Score: -0.30\n",
      "\n",
      "Episode 475000\tAverage Score: -0.30 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 480000\tAverage Score: -0.36\n",
      "\n",
      "Episode 480000\tAverage Score: -0.36 0.3\n",
      "state:  [62 42 44 48 55 73 77 92]\n",
      "action:  36\n",
      "Episode 485000\tAverage Score: -0.15\n",
      "\n",
      "Episode 485000\tAverage Score: -0.15 0.3\n",
      "state:  [65 42 44 48 55 73 77 92]\n",
      "action:  5\n",
      "Episode 490000\tAverage Score: -0.26\n",
      "\n",
      "Episode 490000\tAverage Score: -0.26 0.3\n",
      "state:  [51 42 44 48 55 73 77 92]\n",
      "action:  15\n",
      "Episode 495000\tAverage Score: -0.23\n",
      "\n",
      "Episode 495000\tAverage Score: -0.23 0.3\n",
      "state:  [33 42 44 48 55 73 77 92]\n",
      "action:  42\n",
      "Episode 500000\tAverage Score: -0.24\n",
      "\n",
      "Episode 500000\tAverage Score: -0.24 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  48\n",
      "Episode 505000\tAverage Score: -0.34\n",
      "\n",
      "Episode 505000\tAverage Score: -0.34 0.3\n",
      "state:  [31 42 44 48 55 73 77 92]\n",
      "action:  70\n",
      "Episode 510000\tAverage Score: -0.15\n",
      "\n",
      "Episode 510000\tAverage Score: -0.15 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 515000\tAverage Score: -0.17\n",
      "\n",
      "Episode 515000\tAverage Score: -0.17 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  44\n",
      "Episode 520000\tAverage Score: -0.26\n",
      "\n",
      "Episode 520000\tAverage Score: -0.26 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  44\n",
      "Episode 525000\tAverage Score: -0.15\n",
      "\n",
      "Episode 525000\tAverage Score: -0.15 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 530000\tAverage Score: -0.17\n",
      "\n",
      "Episode 530000\tAverage Score: -0.17 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 535000\tAverage Score: -0.23\n",
      "\n",
      "Episode 535000\tAverage Score: -0.23 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 540000\tAverage Score: -0.21\n",
      "\n",
      "Episode 540000\tAverage Score: -0.21 0.3\n",
      "state:  [81 42 44 48 55 73 77 92]\n",
      "action:  87\n",
      "Episode 545000\tAverage Score: -0.23\n",
      "\n",
      "Episode 545000\tAverage Score: -0.23 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  55\n",
      "Episode 550000\tAverage Score: -0.32\n",
      "\n",
      "Episode 550000\tAverage Score: -0.32 0.3\n",
      "state:  [37 42 44 48 55 73 77 92]\n",
      "action:  10\n",
      "Episode 555000\tAverage Score: -0.21\n",
      "\n",
      "Episode 555000\tAverage Score: -0.21 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  55\n",
      "Episode 560000\tAverage Score: -0.25\n",
      "\n",
      "Episode 560000\tAverage Score: -0.25 0.3\n",
      "state:  [62 42 44 48 55 73 77 92]\n",
      "action:  32\n",
      "Episode 565000\tAverage Score: -0.22\n",
      "\n",
      "Episode 565000\tAverage Score: -0.22 0.3\n",
      "state:  [51 42 44 48 55 73 77 92]\n",
      "action:  87\n",
      "Episode 570000\tAverage Score: -0.24\n",
      "\n",
      "Episode 570000\tAverage Score: -0.24 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  77\n",
      "Episode 575000\tAverage Score: -0.15\n",
      "\n",
      "Episode 575000\tAverage Score: -0.15 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  48\n",
      "Episode 580000\tAverage Score: -0.25\n",
      "\n",
      "Episode 580000\tAverage Score: -0.25 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 585000\tAverage Score: -0.19\n",
      "\n",
      "Episode 585000\tAverage Score: -0.19 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 590000\tAverage Score: -0.21\n",
      "\n",
      "Episode 590000\tAverage Score: -0.21 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  48\n",
      "Episode 595000\tAverage Score: -0.23\n",
      "\n",
      "Episode 595000\tAverage Score: -0.23 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 600000\tAverage Score: -0.25\n",
      "\n",
      "Episode 600000\tAverage Score: -0.25 0.3\n",
      "state:  [33 42 44 48 55 73 77 92]\n",
      "action:  61\n",
      "Episode 605000\tAverage Score: -0.22\n",
      "\n",
      "Episode 605000\tAverage Score: -0.22 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 610000\tAverage Score: -0.31\n",
      "\n",
      "Episode 610000\tAverage Score: -0.31 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  55\n",
      "Episode 615000\tAverage Score: -0.35\n",
      "\n",
      "Episode 615000\tAverage Score: -0.35 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  48\n",
      "Episode 620000\tAverage Score: -0.33\n",
      "\n",
      "Episode 620000\tAverage Score: -0.33 0.3\n",
      "state:  [65 42 44 48 55 73 77 92]\n",
      "action:  86\n",
      "Episode 625000\tAverage Score: -0.22\n",
      "\n",
      "Episode 625000\tAverage Score: -0.22 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  44\n",
      "Episode 630000\tAverage Score: -0.27\n",
      "\n",
      "Episode 630000\tAverage Score: -0.27 0.3\n",
      "state:  [31 42 44 48 55 73 77 92]\n",
      "action:  71\n",
      "Episode 635000\tAverage Score: -0.20\n",
      "\n",
      "Episode 635000\tAverage Score: -0.20 0.3\n",
      "state:  [37 42 44 48 55 73 77 92]\n",
      "action:  41\n",
      "Episode 640000\tAverage Score: -0.25\n",
      "\n",
      "Episode 640000\tAverage Score: -0.25 0.3\n",
      "state:  [31 42 44 48 55 73 77 92]\n",
      "action:  95\n",
      "Episode 645000\tAverage Score: -0.37\n",
      "\n",
      "Episode 645000\tAverage Score: -0.37 0.3\n",
      "state:  [33 42 44 48 55 73 77 92]\n",
      "action:  51\n",
      "Episode 650000\tAverage Score: -0.22\n",
      "\n",
      "Episode 650000\tAverage Score: -0.22 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  48\n",
      "Episode 655000\tAverage Score: -0.30\n",
      "\n",
      "Episode 655000\tAverage Score: -0.30 0.3\n",
      "state:  [37 42 44 48 55 73 77 92]\n",
      "action:  52\n",
      "Episode 660000\tAverage Score: -0.22\n",
      "\n",
      "Episode 660000\tAverage Score: -0.22 0.3\n",
      "state:  [37 42 44 48 55 73 77 92]\n",
      "action:  33\n",
      "Episode 665000\tAverage Score: -0.32\n",
      "\n",
      "Episode 665000\tAverage Score: -0.32 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 670000\tAverage Score: -0.28\n",
      "\n",
      "Episode 670000\tAverage Score: -0.28 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  55\n",
      "Episode 675000\tAverage Score: -0.25\n",
      "\n",
      "Episode 675000\tAverage Score: -0.25 0.3\n",
      "state:  [62 42 44 48 55 73 77 92]\n",
      "action:  20\n",
      "Episode 680000\tAverage Score: -0.31\n",
      "\n",
      "Episode 680000\tAverage Score: -0.31 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 685000\tAverage Score: -0.25\n",
      "\n",
      "Episode 685000\tAverage Score: -0.25 0.3\n",
      "state:  [65 42 44 48 55 73 77 92]\n",
      "action:  3\n",
      "Episode 690000\tAverage Score: -0.26\n",
      "\n",
      "Episode 690000\tAverage Score: -0.26 0.3\n",
      "state:  [62 42 44 48 55 73 77 92]\n",
      "action:  42\n",
      "Episode 695000\tAverage Score: -0.13\n",
      "\n",
      "Episode 695000\tAverage Score: -0.13 0.3\n",
      "state:  [31 42 44 48 55 73 77 92]\n",
      "action:  59\n",
      "Episode 700000\tAverage Score: -0.15\n",
      "\n",
      "Episode 700000\tAverage Score: -0.15 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  44\n",
      "Episode 705000\tAverage Score: -0.32\n",
      "\n",
      "Episode 705000\tAverage Score: -0.32 0.3\n",
      "state:  [81 42 44 48 55 73 77 92]\n",
      "action:  23\n",
      "Episode 710000\tAverage Score: -0.20\n",
      "\n",
      "Episode 710000\tAverage Score: -0.20 0.3\n",
      "state:  [37 42 44 48 55 73 77 92]\n",
      "action:  33\n",
      "Episode 715000\tAverage Score: -0.21\n",
      "\n",
      "Episode 715000\tAverage Score: -0.21 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 720000\tAverage Score: -0.31\n",
      "\n",
      "Episode 720000\tAverage Score: -0.31 0.3\n",
      "state:  [81 42 44 48 55 73 77 92]\n",
      "action:  66\n",
      "Episode 725000\tAverage Score: -0.32\n",
      "\n",
      "Episode 725000\tAverage Score: -0.32 0.3\n",
      "state:  [81 42 44 48 55 73 77 92]\n",
      "action:  7\n",
      "Episode 730000\tAverage Score: -0.23\n",
      "\n",
      "Episode 730000\tAverage Score: -0.23 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  55\n",
      "Episode 735000\tAverage Score: -0.22\n",
      "\n",
      "Episode 735000\tAverage Score: -0.22 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 740000\tAverage Score: -0.16\n",
      "\n",
      "Episode 740000\tAverage Score: -0.16 0.3\n",
      "state:  [81 42 44 48 55 73 77 92]\n",
      "action:  22\n",
      "Episode 745000\tAverage Score: -0.34\n",
      "\n",
      "Episode 745000\tAverage Score: -0.34 0.3\n",
      "state:  [62 42 44 48 55 73 77 92]\n",
      "action:  71\n",
      "Episode 750000\tAverage Score: -0.23\n",
      "\n",
      "Episode 750000\tAverage Score: -0.23 0.3\n",
      "state:  [51 42 44 48 55 73 77 92]\n",
      "action:  50\n",
      "Episode 755000\tAverage Score: -0.26\n",
      "\n",
      "Episode 755000\tAverage Score: -0.26 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  77\n",
      "Episode 760000\tAverage Score: -0.26\n",
      "\n",
      "Episode 760000\tAverage Score: -0.26 0.3\n",
      "state:  [51 42 44 48 55 73 77 92]\n",
      "action:  37\n",
      "Episode 765000\tAverage Score: -0.34\n",
      "\n",
      "Episode 765000\tAverage Score: -0.34 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  44\n",
      "Episode 770000\tAverage Score: -0.22\n",
      "\n",
      "Episode 770000\tAverage Score: -0.22 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 775000\tAverage Score: -0.26\n",
      "\n",
      "Episode 775000\tAverage Score: -0.26 0.3\n",
      "state:  [65 42 44 48 55 73 77 92]\n",
      "action:  39\n",
      "Episode 780000\tAverage Score: -0.26\n",
      "\n",
      "Episode 780000\tAverage Score: -0.26 0.3\n",
      "state:  [62 42 44 48 55 73 77 92]\n",
      "action:  14\n",
      "Episode 785000\tAverage Score: -0.24\n",
      "\n",
      "Episode 785000\tAverage Score: -0.24 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  48\n",
      "Episode 790000\tAverage Score: -0.23\n",
      "\n",
      "Episode 790000\tAverage Score: -0.23 0.3\n",
      "state:  [31 42 44 48 55 73 77 92]\n",
      "action:  81\n",
      "Episode 795000\tAverage Score: -0.23\n",
      "\n",
      "Episode 795000\tAverage Score: -0.23 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  44\n",
      "Episode 800000\tAverage Score: -0.30\n",
      "\n",
      "Episode 800000\tAverage Score: -0.30 0.3\n",
      "state:  [51 42 44 48 55 73 77 92]\n",
      "action:  43\n",
      "Episode 805000\tAverage Score: -0.25\n",
      "\n",
      "Episode 805000\tAverage Score: -0.25 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 810000\tAverage Score: -0.33\n",
      "\n",
      "Episode 810000\tAverage Score: -0.33 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 815000\tAverage Score: -0.27\n",
      "\n",
      "Episode 815000\tAverage Score: -0.27 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 820000\tAverage Score: -0.30\n",
      "\n",
      "Episode 820000\tAverage Score: -0.30 0.3\n",
      "state:  [81 42 44 48 55 73 77 92]\n",
      "action:  90\n",
      "Episode 825000\tAverage Score: -0.21\n",
      "\n",
      "Episode 825000\tAverage Score: -0.21 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 830000\tAverage Score: -0.19\n",
      "\n",
      "Episode 830000\tAverage Score: -0.19 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  55\n",
      "Episode 835000\tAverage Score: -0.27\n",
      "\n",
      "Episode 835000\tAverage Score: -0.27 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  44\n",
      "Episode 840000\tAverage Score: -0.22\n",
      "\n",
      "Episode 840000\tAverage Score: -0.22 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  77\n",
      "Episode 845000\tAverage Score: -0.26\n",
      "\n",
      "Episode 845000\tAverage Score: -0.26 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  55\n",
      "Episode 850000\tAverage Score: -0.26\n",
      "\n",
      "Episode 850000\tAverage Score: -0.26 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 855000\tAverage Score: -0.30\n",
      "\n",
      "Episode 855000\tAverage Score: -0.30 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  48\n",
      "Episode 860000\tAverage Score: -0.33\n",
      "\n",
      "Episode 860000\tAverage Score: -0.33 0.3\n",
      "state:  [33 42 44 48 55 73 77 92]\n",
      "action:  42\n",
      "Episode 865000\tAverage Score: -0.25\n",
      "\n",
      "Episode 865000\tAverage Score: -0.25 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  77\n",
      "Episode 870000\tAverage Score: -0.30\n",
      "\n",
      "Episode 870000\tAverage Score: -0.30 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  48\n",
      "Episode 875000\tAverage Score: -0.22\n",
      "\n",
      "Episode 875000\tAverage Score: -0.22 0.3\n",
      "state:  [37 42 44 48 55 73 77 92]\n",
      "action:  29\n",
      "Episode 880000\tAverage Score: -0.31\n",
      "\n",
      "Episode 880000\tAverage Score: -0.31 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  48\n",
      "Episode 885000\tAverage Score: -0.20\n",
      "\n",
      "Episode 885000\tAverage Score: -0.20 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  77\n",
      "Episode 890000\tAverage Score: -0.24\n",
      "\n",
      "Episode 890000\tAverage Score: -0.24 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 895000\tAverage Score: -0.21\n",
      "\n",
      "Episode 895000\tAverage Score: -0.21 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  48\n",
      "Episode 900000\tAverage Score: -0.27\n",
      "\n",
      "Episode 900000\tAverage Score: -0.27 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  44\n",
      "Episode 905000\tAverage Score: -0.21\n",
      "\n",
      "Episode 905000\tAverage Score: -0.21 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 910000\tAverage Score: -0.27\n",
      "\n",
      "Episode 910000\tAverage Score: -0.27 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  55\n",
      "Episode 915000\tAverage Score: -0.19\n",
      "\n",
      "Episode 915000\tAverage Score: -0.19 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  77\n",
      "Episode 920000\tAverage Score: -0.17\n",
      "\n",
      "Episode 920000\tAverage Score: -0.17 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 925000\tAverage Score: -0.37\n",
      "\n",
      "Episode 925000\tAverage Score: -0.37 0.3\n",
      "state:  [81 42 44 48 55 73 77 92]\n",
      "action:  76\n",
      "Episode 930000\tAverage Score: -0.31\n",
      "\n",
      "Episode 930000\tAverage Score: -0.31 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  77\n",
      "Episode 935000\tAverage Score: -0.21\n",
      "\n",
      "Episode 935000\tAverage Score: -0.21 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  48\n",
      "Episode 940000\tAverage Score: -0.27\n",
      "\n",
      "Episode 940000\tAverage Score: -0.27 0.3\n",
      "state:  [81 42 44 48 55 73 77 92]\n",
      "action:  62\n",
      "Episode 945000\tAverage Score: -0.33\n",
      "\n",
      "Episode 945000\tAverage Score: -0.33 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  44\n",
      "Episode 950000\tAverage Score: -0.23\n",
      "\n",
      "Episode 950000\tAverage Score: -0.23 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  42\n",
      "Episode 955000\tAverage Score: -0.23\n",
      "\n",
      "Episode 955000\tAverage Score: -0.23 0.3\n",
      "state:  [65 42 44 48 55 73 77 92]\n",
      "action:  51\n",
      "Episode 960000\tAverage Score: -0.22\n",
      "\n",
      "Episode 960000\tAverage Score: -0.22 0.3\n",
      "state:  [81 42 44 48 55 73 77 92]\n",
      "action:  67\n",
      "Episode 965000\tAverage Score: -0.24\n",
      "\n",
      "Episode 965000\tAverage Score: -0.24 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  77\n",
      "Episode 970000\tAverage Score: -0.21\n",
      "\n",
      "Episode 970000\tAverage Score: -0.21 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 975000\tAverage Score: -0.22\n",
      "\n",
      "Episode 975000\tAverage Score: -0.22 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  73\n",
      "Episode 980000\tAverage Score: -0.28\n",
      "\n",
      "Episode 980000\tAverage Score: -0.28 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  44\n",
      "Episode 985000\tAverage Score: -0.20\n",
      "\n",
      "Episode 985000\tAverage Score: -0.20 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  77\n",
      "Episode 990000\tAverage Score: -0.24\n",
      "\n",
      "Episode 990000\tAverage Score: -0.24 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  92\n",
      "Episode 995000\tAverage Score: -0.34\n",
      "\n",
      "Episode 995000\tAverage Score: -0.34 0.3\n",
      "state:  [62 42 44 48 55 73 77 92]\n",
      "action:  69\n",
      "Episode 1000000\tAverage Score: -0.27\n",
      "\n",
      "Episode 1000000\tAverage Score: -0.27 0.3\n",
      "state:  [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "action:  48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def dqn(n_episodes=1000000, max_t=1, eps_start=0.9, eps_end=0.3, eps_decay=0.99):\n",
    "   \n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    consequetive_episode  = []\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset() #note state is atuple with a state and normalized state\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps) #select normalized state\n",
    "            next_state, reward,done = env.env_behaviour(state, action)\n",
    "            #if reward == 0.1:\n",
    "                #print(\"\\nstate : \",state, \"\\naction: \", action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        \n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 5000 == 0:\n",
    "            print('\\n')\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)),eps)\n",
    "            print(\"state: \", state)\n",
    "            print(\"action: \", action)\n",
    "        \n",
    "        if i_episode % 1000 == 0:\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            \n",
    "        if np.mean(scores_window) >=0.1: #and len(consequetive_episode)>7: # and i_episode >= 1000:\n",
    "            \n",
    "            print('\\nnp.mean(scores_window): ', np.mean(scores_window))\n",
    "            if len(consequetive_episode) == 0:\n",
    "                consequetive_episode.append(i_episode)\n",
    "\n",
    "            \n",
    "            else: \n",
    "                if consequetive_episode[-1] == i_episode -1:\n",
    "                    consequetive_episode.append(i_episode)\n",
    "                else:\n",
    "                    consequetive_episode = []\n",
    "\n",
    "\n",
    "        if len(consequetive_episode)>7:\n",
    "\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "            \n",
    "    return scores\n",
    "\n",
    "scores = dqn()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
