{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn_agent_execution import Agent\n",
    "from env_setup import EnvGrid\n",
    "\n",
    "env = EnvGrid (8 , 4)\n",
    "#agent = Agent(state_size=6, action_size=2, seed=0)\n",
    "agent = Agent(state_size=9, action_size=34, seed=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action : \n",
      " 24\n",
      "state:\n",
      " [ 8.90625  6.5625   7.5      7.03125 10.3125  11.25    10.78125  8.4375\n",
      "  9.375  ]\n",
      "self.current_state_list:\n",
      " [19, 14, 16, 15, 22, 24, 23, 18, 20]\n",
      "Episode 1\tAverage Score: 0.00action : \n",
      " 27\n",
      "state:\n",
      " [10.3125   7.96875  8.90625  8.4375  11.71875 12.65625 12.1875   9.84375\n",
      " 10.78125]\n",
      "self.current_state_list:\n",
      " [22, 17, 19, 18, 25, 27, 26, 21, 23]\n",
      "Episode 2\tAverage Score: 0.00action : \n",
      " 12\n",
      "state:\n",
      " [3.28125 0.9375  1.875   1.40625 4.6875  5.625   5.15625 2.8125  3.75   ]\n",
      "self.current_state_list:\n",
      " [7, 2, 4, 3, 10, 12, 11, 6, 8]\n",
      "Episode 3\tAverage Score: 0.00action : \n",
      " 23\n",
      "state:\n",
      " [ 8.4375   6.09375  7.03125  6.5625   9.84375 10.78125 10.3125   7.96875\n",
      "  8.90625]\n",
      "self.current_state_list:\n",
      " [18, 13, 15, 14, 21, 23, 22, 17, 19]\n",
      "Episode 4\tAverage Score: 0.00action : \n",
      " 18\n",
      "state:\n",
      " [6.09375 8.4375  7.96875 4.6875  4.21875 6.5625  0.      0.      0.     ]\n",
      "self.current_state_list:\n",
      " [13, 18, 17, 10, 9, 14]\n",
      "Episode 5\tAverage Score: 0.00"
     ]
    }
   ],
   "source": [
    "def dqn(n_episodes=5, max_t=1, eps_start=0.0, eps_end=0.0, eps_decay=0.9):\n",
    "#def dqn(n_episodes=10000000, max_t=10, eps_start=1.0, eps_end=0.3, eps_decay=0.999995):\n",
    "    \n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        #print(\"state: \\n\",state)\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            \n",
    "            print(\"action : \\n\", action)\n",
    "            print(\"state:\\n\", state*15) #(state* num of vertical grids * number of horizontal grids)\n",
    "            next_state, reward, done = env.env_behaviour(state, action)\n",
    "            #print('reward,state,next_state, action',reward, state, next_state, action)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")     \n",
    "    return scores\n",
    "\n",
    "scores = dqn()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
